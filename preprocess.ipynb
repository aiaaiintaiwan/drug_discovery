{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing import sequence, text\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import rdkit\n",
    "from tqdm import tqdm\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder path\n",
    "DATASET_DIR = './datasets/zinc-preprocess/'\n",
    "MAPPING_DIR = './mapping/'\n",
    "\n",
    "# SMILES path\n",
    "SMILES_FILENAME = '250k_rndm_zinc_drugs_clean' # do not include '.smi' extension\n",
    "\n",
    "# MAPPING TABLE PATH\n",
    "MAPPING_TABLE_DIR = MAPPING_DIR + SMILES_FILENAME\n",
    "WORD_IDX_FILENAME = 'word_to_idx.json'\n",
    "IDX_WORD_FILENAME = 'idx_to_word.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Directory alread exists\n"
     ]
    }
   ],
   "source": [
    "# Create mapping folder\n",
    "if not os.path.isdir(MAPPING_TABLE_DIR):\n",
    "    try:\n",
    "        os.makedirs(MAPPING_TABLE_DIR)\n",
    "    except OSError:\n",
    "        logging.error(\"Creation of the directory failed\")\n",
    "    else:\n",
    "        print(\"Successfully created the directory\")\n",
    "else:\n",
    "    logging.warning(\"Directory alread exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 319616985/319616985 [5:27:12<00:00, 16279.95it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of SMILES: 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "# note: when char_tokenizerl=True, then filters not working! so we need split \\n\n",
    "tokenizer = text.Tokenizer(char_level=True, lower=False)\n",
    "\n",
    "# Load data\n",
    "with open(DATASET_DIR + SMILES_FILENAME + '.smi', 'r') as f:\n",
    "    for smiles in tqdm(f.readlines()):\n",
    "        smiles = smiles.split('\\n')[0]\n",
    "        tokenizer.fit_on_texts(smiles)\n",
    "        \n",
    "    word_index = tokenizer.word_index\n",
    "    print('Total number of SMILES: {}'.format(len(word_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1,\n",
       " '@': 2,\n",
       " '[': 3,\n",
       " ']': 4,\n",
       " 'H': 5,\n",
       " '1': 6,\n",
       " '2': 7,\n",
       " 'O': 8,\n",
       " '(': 9,\n",
       " ')': 10,\n",
       " 'N': 11,\n",
       " '=': 12,\n",
       " '3': 13,\n",
       " 'l': 14,\n",
       " 'S': 15,\n",
       " '#': 16,\n",
       " 'B': 17,\n",
       " 'r': 18,\n",
       " 'F': 19,\n",
       " '4': 20,\n",
       " '/': 21,\n",
       " 'c': 22,\n",
       " '+': 23,\n",
       " '\\\\': 24,\n",
       " 'P': 25,\n",
       " '-': 26,\n",
       " 'I': 27,\n",
       " 'n': 28,\n",
       " 'o': 29,\n",
       " '<START>': 30,\n",
       " '<PAD>': 31,\n",
       " '<EOL>': 32}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add prefix\n",
    "SMILES_DIM = len(word_index)\n",
    "word_index['<START>'] = SMILES_DIM + 1 \n",
    "word_index['<PAD>'] = SMILES_DIM + 2\n",
    "word_index['<EOL>'] = SMILES_DIM + 3 \n",
    "\n",
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'C',\n",
       " 2: '@',\n",
       " 3: '[',\n",
       " 4: ']',\n",
       " 5: 'H',\n",
       " 6: '1',\n",
       " 7: '2',\n",
       " 8: 'O',\n",
       " 9: '(',\n",
       " 10: ')',\n",
       " 11: 'N',\n",
       " 12: '=',\n",
       " 13: '3',\n",
       " 14: 'l',\n",
       " 15: 'S',\n",
       " 16: '#',\n",
       " 17: 'B',\n",
       " 18: 'r',\n",
       " 19: 'F',\n",
       " 20: '4',\n",
       " 21: '/',\n",
       " 22: 'c',\n",
       " 23: '+',\n",
       " 24: '\\\\',\n",
       " 25: 'P',\n",
       " 26: '-',\n",
       " 27: 'I',\n",
       " 28: 'n',\n",
       " 29: 'o',\n",
       " 30: '<START>',\n",
       " 31: '<PAD>',\n",
       " 32: '<EOL>'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# index to word\n",
    "index_word = {index:word for word, index in word_index.items()}\n",
    "index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "## using tokenizer class method\n",
    "tokenizer_json = tokenizer.to_json()\n",
    "with open(MAPPING_TABLE_DIR + '/tokenizer.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(tokenizer_json, ensure_ascii=False))\n",
    "\n",
    "## old school|\n",
    "with open(MAPPING_TABLE_DIR + '/' + WORD_IDX_FILENAME, 'w') as wi, \\\n",
    "    open(MAPPING_TABLE_DIR + '/' + IDX_WORD_FILENAME, 'w') as iw:\n",
    "        wi.write(json.dumps(word_index))\n",
    "        iw.write(json.dumps(index_word))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
